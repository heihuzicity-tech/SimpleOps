#!/usr/bin/env python3
"""
å¿«é€Ÿæ€§èƒ½éªŒè¯æµ‹è¯•
éªŒè¯ç´¢å¼•ä¼˜åŒ–åçš„æ€§èƒ½æ”¹è¿›
"""

import json
import requests
import time
import statistics
from typing import List, Dict

class QuickPerformanceTest:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.session = requests.Session()
        self.token = None
        
    def login(self) -> bool:
        try:
            response = self.session.post(
                f"{self.base_url}/api/v1/auth/login",
                json={"username": "admin", "password": "admin123"}
            )
            response.raise_for_status()
            data = response.json()
            self.token = data["data"]["access_token"]
            self.session.headers.update({"Authorization": f"Bearer {self.token}"})
            return True
        except Exception as e:
            print(f"âŒ ç™»å½•å¤±è´¥: {e}")
            return False
    
    def measure_response_time(self, endpoint: str, params: Dict = None) -> float:
        """æµ‹é‡å•æ¬¡APIå“åº”æ—¶é—´"""
        start_time = time.perf_counter()
        try:
            response = self.session.get(f"{self.base_url}{endpoint}", params=params)
            response.raise_for_status()
            end_time = time.perf_counter()
            return (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥ {endpoint}: {e}")
            return -1
    
    def test_api_performance(self, endpoint: str, name: str, iterations: int = 10) -> Dict:
        """æµ‹è¯•APIæ€§èƒ½"""
        print(f"  æµ‹è¯• {name}...")
        times = []
        
        for i in range(iterations):
            response_time = self.measure_response_time(endpoint)
            if response_time > 0:
                times.append(response_time)
        
        if not times:
            return {"error": "æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†"}
        
        result = {
            "avg_ms": statistics.mean(times),
            "min_ms": min(times),
            "max_ms": max(times),
            "median_ms": statistics.median(times),
            "count": len(times),
            "success_rate": len(times) / iterations * 100
        }
        
        status = "âœ…" if result["avg_ms"] < 10 else "âŒ"
        print(f"    {status} å¹³å‡: {result['avg_ms']:.2f}ms, æœ€å°: {result['min_ms']:.2f}ms, æœ€å¤§: {result['max_ms']:.2f}ms")
        
        return result

def main():
    print("=== å¿«é€Ÿæ€§èƒ½éªŒè¯æµ‹è¯• ===")
    print("éªŒè¯ç´¢å¼•ä¼˜åŒ–åçš„æ€§èƒ½æ”¹è¿›\n")
    
    tester = QuickPerformanceTest()
    
    if not tester.login():
        return
    
    # æµ‹è¯•ä¸»è¦APIæ¥å£çš„æ€§èƒ½
    test_cases = [
        ("/api/v1/command-filter/commands?page=1&page_size=10", "å‘½ä»¤åˆ—è¡¨æŸ¥è¯¢"),
        ("/api/v1/command-filter/policies?page=1&page_size=10", "ç­–ç•¥åˆ—è¡¨æŸ¥è¯¢"),
        ("/api/v1/command-filter/command-groups?page=1&page_size=10", "å‘½ä»¤ç»„åˆ—è¡¨æŸ¥è¯¢"),
        ("/api/v1/command-filter/intercept-logs?page=1&page_size=10", "æ‹¦æˆªæ—¥å¿—æŸ¥è¯¢"),
        ("/api/v1/command-filter/commands?page=1&page_size=50", "å¤§é¡µé¢å‘½ä»¤æŸ¥è¯¢"),
        ("/api/v1/command-filter/policies?page=1&page_size=20", "å¤§é¡µé¢ç­–ç•¥æŸ¥è¯¢"),
    ]
    
    results = {}
    passed_count = 0
    total_count = len(test_cases)
    
    print("1. ğŸ” ç´¢å¼•ä¼˜åŒ–åæ€§èƒ½æµ‹è¯•")
    
    for endpoint, name in test_cases:
        result = tester.test_api_performance(endpoint, name, iterations=15)
        results[name] = result
        
        if "error" not in result and result["avg_ms"] < 10:
            passed_count += 1
    
    # ç®€å•çš„å¹¶å‘æµ‹è¯•
    print("\n2. ğŸš€ ç®€å•å¹¶å‘æµ‹è¯•")
    concurrent_results = {}
    
    # 5ä¸ªå¹¶å‘è¯·æ±‚æµ‹è¯•
    import threading
    
    def concurrent_request():
        return tester.measure_response_time("/api/v1/command-filter/commands?page=1&page_size=10")
    
    print("  æ‰§è¡Œ5ä¸ªå¹¶å‘è¯·æ±‚...")
    threads = []
    concurrent_times = []
    
    start_time = time.perf_counter()
    
    for _ in range(5):
        thread = threading.Thread(target=lambda: concurrent_times.append(concurrent_request()))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    end_time = time.perf_counter()
    total_time = (end_time - start_time) * 1000
    
    valid_times = [t for t in concurrent_times if t > 0]
    
    if valid_times:
        concurrent_results["parallel_commands"] = {
            "avg_ms": statistics.mean(valid_times),
            "max_ms": max(valid_times),
            "total_time_ms": total_time,
            "success_rate": len(valid_times) / 5 * 100
        }
        
        status = "âœ…" if statistics.mean(valid_times) < 50 else "âŒ"  # å¹¶å‘æ—¶æ”¾å®½è¦æ±‚åˆ°50ms
        print(f"    {status} å¹¶å‘å¹³å‡å“åº”æ—¶é—´: {statistics.mean(valid_times):.2f}ms")
        print(f"    æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ms")
        print(f"    æˆåŠŸç‡: {len(valid_times)/5*100:.1f}%")
    
    # æ±‡æ€»ç»“æœ
    print("\n=== æ€§èƒ½æµ‹è¯•æ±‡æ€» ===")
    print(f"é€šè¿‡æµ‹è¯•: {passed_count}/{total_count} ({passed_count/total_count*100:.1f}%)")
    
    # ä¸ä¹‹å‰çš„åŸºçº¿æ¯”è¾ƒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    baseline_comparison = {
        "å‘½ä»¤åˆ—è¡¨æŸ¥è¯¢": 19.03,
        "ç­–ç•¥åˆ—è¡¨æŸ¥è¯¢": 24.95,
        "å‘½ä»¤ç»„åˆ—è¡¨æŸ¥è¯¢": 17.00,
        "æ‹¦æˆªæ—¥å¿—æŸ¥è¯¢": 12.82
    }
    
    print("\næ€§èƒ½æ”¹è¿›å¯¹æ¯”:")
    improvements = []
    
    for name, baseline_time in baseline_comparison.items():
        if name in results and "error" not in results[name]:
            current_time = results[name]["avg_ms"]
            improvement = ((baseline_time - current_time) / baseline_time) * 100
            improvements.append(improvement)
            
            if improvement > 0:
                print(f"  âœ… {name}: {baseline_time:.2f}ms â†’ {current_time:.2f}ms (æ”¹è¿› {improvement:.1f}%)")
            else:
                print(f"  âŒ {name}: {baseline_time:.2f}ms â†’ {current_time:.2f}ms (é€€åŒ– {abs(improvement):.1f}%)")
    
    if improvements:
        avg_improvement = statistics.mean(improvements)
        print(f"\nå¹³å‡æ€§èƒ½æ”¹è¿›: {avg_improvement:.1f}%")
        
        if avg_improvement > 20:
            print("ğŸ‰ ç´¢å¼•ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼")
        elif avg_improvement > 0:
            print("âœ… ç´¢å¼•ä¼˜åŒ–æœ‰ä¸€å®šæ•ˆæœ")
        else:
            print("âš ï¸ ç´¢å¼•ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # ä¿å­˜ç»“æœ
    all_results = {
        "performance_tests": results,
        "concurrent_tests": concurrent_results,
        "summary": {
            "passed_tests": passed_count,
            "total_tests": total_count,
            "pass_rate": passed_count / total_count * 100,
            "average_improvement": statistics.mean(improvements) if improvements else 0
        },
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open('.specs/å‘½ä»¤ç­–ç•¥åŠŸèƒ½å¼€å‘/quick-performance-test-6.3.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nè¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: quick-performance-test-6.3.json")

if __name__ == "__main__":
    main()