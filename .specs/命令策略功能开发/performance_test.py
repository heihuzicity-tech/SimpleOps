#!/usr/bin/env python3
"""
å‘½ä»¤ç­–ç•¥æœåŠ¡æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•å¤§é‡ç­–ç•¥ã€å‘½ä»¤å’Œå¹¶å‘è®¿é—®ä¸‹çš„ç³»ç»Ÿæ€§èƒ½
"""

import json
import requests
import time
import threading
import statistics
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple

class PerformanceTestClient:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.session = requests.Session()
        self.token = None
        
    def login(self, username: str = "admin", password: str = "admin123") -> bool:
        """ç™»å½•è·å–token"""
        try:
            response = self.session.post(
                f"{self.base_url}/api/v1/auth/login",
                json={"username": username, "password": password}
            )
            response.raise_for_status()
            data = response.json()
            self.token = data["data"]["access_token"]
            self.session.headers.update({"Authorization": f"Bearer {self.token}"})
            return True
        except Exception as e:
            print(f"âŒ ç™»å½•å¤±è´¥: {e}")
            return False
    
    def create_batch_commands(self, count: int) -> List[int]:
        """æ‰¹é‡åˆ›å»ºæµ‹è¯•å‘½ä»¤"""
        command_patterns = [
            ("rm.*", "regex"),
            ("sudo.*", "regex"), 
            ("^shutdown.*", "regex"),
            ("chmod.*777.*", "regex"),
            (".*passwd.*", "regex"),
            ("find.*-exec.*rm.*", "regex"),
            ("(wget|curl).*", "regex"),
            ("dd.*of=/dev/.*", "regex"),
            ("iptables.*-j.*DROP.*", "regex"),
            ("mount.*", "regex")
        ]
        
        created_ids = []
        print(f"åˆ›å»º {count} ä¸ªæµ‹è¯•å‘½ä»¤...")
        
        for i in range(count):
            pattern, cmd_type = command_patterns[i % len(command_patterns)]
            name = f"{pattern}_{i}"
            description = f"æ€§èƒ½æµ‹è¯•å‘½ä»¤ {i+1}"
            
            try:
                response = self.session.post(
                    f"{self.base_url}/api/v1/command-filter/commands",
                    json={"name": name, "type": cmd_type, "description": description}
                )
                if response.status_code == 200:
                    data = response.json()
                    if "id" in data:
                        created_ids.append(data["id"])
                        if (i + 1) % 10 == 0:
                            print(f"  å·²åˆ›å»º {i+1}/{count} ä¸ªå‘½ä»¤")
                else:
                    print(f"  åˆ›å»ºå‘½ä»¤å¤±è´¥ {i+1}: {response.text}")
            except Exception as e:
                print(f"  åˆ›å»ºå‘½ä»¤å¼‚å¸¸ {i+1}: {e}")
        
        print(f"âœ… æˆåŠŸåˆ›å»º {len(created_ids)} ä¸ªå‘½ä»¤")
        return created_ids
    
    def create_batch_policies(self, count: int, command_ids: List[int]) -> List[int]:
        """æ‰¹é‡åˆ›å»ºæµ‹è¯•ç­–ç•¥"""
        created_ids = []
        print(f"åˆ›å»º {count} ä¸ªæµ‹è¯•ç­–ç•¥...")
        
        commands_per_policy = max(1, len(command_ids) // count)
        
        for i in range(count):
            policy_name = f"æ€§èƒ½æµ‹è¯•ç­–ç•¥_{i+1}"
            description = f"ç”¨äºæ€§èƒ½æµ‹è¯•çš„ç­–ç•¥ {i+1}"
            
            try:
                # åˆ›å»ºç­–ç•¥
                response = self.session.post(
                    f"{self.base_url}/api/v1/command-filter/policies",
                    json={"name": policy_name, "description": description, "enabled": True}
                )
                
                if response.status_code == 200:
                    policy_data = response.json()
                    if "id" in policy_data:
                        policy_id = policy_data["id"]
                        created_ids.append(policy_id)
                        
                        # ç»‘å®šå‘½ä»¤åˆ°ç­–ç•¥
                        start_idx = i * commands_per_policy
                        end_idx = min(start_idx + commands_per_policy, len(command_ids))
                        policy_commands = command_ids[start_idx:end_idx]
                        
                        if policy_commands:
                            bind_response = self.session.post(
                                f"{self.base_url}/api/v1/command-filter/policies/{policy_id}/bind-commands",
                                json={"command_ids": policy_commands, "command_group_ids": []}
                            )
                        
                        # ç»‘å®šadminç”¨æˆ·
                        user_response = self.session.post(
                            f"{self.base_url}/api/v1/command-filter/policies/{policy_id}/bind-users",
                            json={"user_ids": [1]}
                        )
                        
                        if (i + 1) % 5 == 0:
                            print(f"  å·²åˆ›å»º {i+1}/{count} ä¸ªç­–ç•¥")
                            
            except Exception as e:
                print(f"  åˆ›å»ºç­–ç•¥å¼‚å¸¸ {i+1}: {e}")
        
        print(f"âœ… æˆåŠŸåˆ›å»º {len(created_ids)} ä¸ªç­–ç•¥")
        return created_ids
    
    def measure_api_response_time(self, endpoint: str, params: Dict = None) -> float:
        """æµ‹é‡APIå“åº”æ—¶é—´"""
        start_time = time.perf_counter()
        try:
            response = self.session.get(f"{self.base_url}{endpoint}", params=params)
            response.raise_for_status()
            end_time = time.perf_counter()
            return (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        except Exception as e:
            end_time = time.perf_counter()
            print(f"APIè¯·æ±‚å¤±è´¥ {endpoint}: {e}")
            return -1
    
    def concurrent_api_test(self, endpoint: str, concurrent_users: int, requests_per_user: int) -> Dict:
        """å¹¶å‘APIæµ‹è¯•"""
        def worker():
            worker_times = []
            for _ in range(requests_per_user):
                response_time = self.measure_api_response_time(endpoint)
                if response_time > 0:
                    worker_times.append(response_time)
                time.sleep(0.01)  # å°é—´éš”é¿å…è¿‡åº¦å‹æµ‹
            return worker_times
        
        print(f"æ‰§è¡Œå¹¶å‘æµ‹è¯•: {concurrent_users} ç”¨æˆ· x {requests_per_user} è¯·æ±‚ = {concurrent_users * requests_per_user} æ€»è¯·æ±‚")
        
        start_time = time.perf_counter()
        all_times = []
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker) for _ in range(concurrent_users)]
            
            for future in as_completed(futures):
                try:
                    worker_times = future.result()
                    all_times.extend(worker_times)
                except Exception as e:
                    print(f"Workeræ‰§è¡Œå¼‚å¸¸: {e}")
        
        end_time = time.perf_counter()
        total_duration = (end_time - start_time) * 1000
        
        if not all_times:
            return {"error": "æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚"}
        
        return {
            "total_requests": len(all_times),
            "total_duration_ms": total_duration,
            "avg_response_time_ms": statistics.mean(all_times),
            "min_response_time_ms": min(all_times),
            "max_response_time_ms": max(all_times),
            "median_response_time_ms": statistics.median(all_times),
            "p95_response_time_ms": statistics.quantiles(all_times, n=20)[18] if len(all_times) > 20 else max(all_times),
            "requests_per_second": len(all_times) / (total_duration / 1000) if total_duration > 0 else 0,
            "success_rate": len(all_times) / (concurrent_users * requests_per_user) * 100
        }

def main():
    print("=== å‘½ä»¤ç­–ç•¥æœåŠ¡æ€§èƒ½æµ‹è¯• ===\n")
    
    client = PerformanceTestClient()
    
    # ç™»å½•
    if not client.login():
        sys.exit(1)
    
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    results = {}
    
    # 1. åŸºçº¿æ€§èƒ½æµ‹è¯•
    print("\n1. ğŸ” åŸºçº¿æ€§èƒ½æµ‹è¯•")
    baseline_tests = [
        ("/api/v1/command-filter/commands?page=1&page_size=10", "å‘½ä»¤åˆ—è¡¨æŸ¥è¯¢"),
        ("/api/v1/command-filter/policies?page=1&page_size=10", "ç­–ç•¥åˆ—è¡¨æŸ¥è¯¢"),
        ("/api/v1/command-filter/command-groups?page=1&page_size=10", "å‘½ä»¤ç»„åˆ—è¡¨æŸ¥è¯¢"),
        ("/api/v1/command-filter/intercept-logs?page=1&page_size=10", "æ‹¦æˆªæ—¥å¿—æŸ¥è¯¢")
    ]
    
    baseline_results = {}
    for endpoint, name in baseline_tests:
        times = []
        print(f"  æµ‹è¯• {name}...")
        for _ in range(10):
            response_time = client.measure_api_response_time(endpoint)
            if response_time > 0:
                times.append(response_time)
        
        if times:
            baseline_results[name] = {
                "avg_ms": statistics.mean(times),
                "min_ms": min(times),
                "max_ms": max(times)
            }
            print(f"    å¹³å‡: {statistics.mean(times):.2f}ms, æœ€å°: {min(times):.2f}ms, æœ€å¤§: {max(times):.2f}ms")
    
    results["baseline"] = baseline_results
    
    # 2. å¤§æ•°æ®é‡æµ‹è¯•
    print("\n2. ğŸ“Š å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•")
    print("åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®...")
    
    # åˆ›å»º100ä¸ªå‘½ä»¤
    command_ids = client.create_batch_commands(50)  # å‡å°‘æ•°é‡é¿å…è¿‡åº¦è´Ÿè½½
    
    # åˆ›å»º20ä¸ªç­–ç•¥
    policy_ids = client.create_batch_policies(10, command_ids)
    
    # æµ‹è¯•å¤§æ•°æ®é‡ä¸‹çš„æŸ¥è¯¢æ€§èƒ½
    large_data_tests = [
        ("/api/v1/command-filter/commands?page=1&page_size=50", "å¤§é‡å‘½ä»¤æŸ¥è¯¢"),
        ("/api/v1/command-filter/policies?page=1&page_size=20", "å¤§é‡ç­–ç•¥æŸ¥è¯¢")
    ]
    
    large_data_results = {}
    for endpoint, name in large_data_tests:
        times = []
        print(f"  æµ‹è¯• {name}...")
        for _ in range(5):
            response_time = client.measure_api_response_time(endpoint)
            if response_time > 0:
                times.append(response_time)
        
        if times:
            large_data_results[name] = {
                "avg_ms": statistics.mean(times),
                "min_ms": min(times),
                "max_ms": max(times)
            }
            print(f"    å¹³å‡: {statistics.mean(times):.2f}ms")
    
    results["large_data"] = large_data_results
    
    # 3. å¹¶å‘æ€§èƒ½æµ‹è¯•
    print("\n3. ğŸš€ å¹¶å‘æ€§èƒ½æµ‹è¯•")
    concurrent_tests = [
        ("/api/v1/command-filter/commands?page=1&page_size=10", 5, 10, "å‘½ä»¤æŸ¥è¯¢å¹¶å‘æµ‹è¯•"),
        ("/api/v1/command-filter/policies?page=1&page_size=10", 3, 10, "ç­–ç•¥æŸ¥è¯¢å¹¶å‘æµ‹è¯•"),
    ]
    
    concurrent_results = {}
    for endpoint, concurrent_users, requests_per_user, name in concurrent_tests:
        print(f"  æ‰§è¡Œ {name}...")
        result = client.concurrent_api_test(endpoint, concurrent_users, requests_per_user)
        concurrent_results[name] = result
        
        if "error" not in result:
            print(f"    å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time_ms']:.2f}ms")
            print(f"    95%å“åº”æ—¶é—´: {result['p95_response_time_ms']:.2f}ms")
            print(f"    QPS: {result['requests_per_second']:.1f}")
            print(f"    æˆåŠŸç‡: {result['success_rate']:.1f}%")
    
    results["concurrent"] = concurrent_results
    
    # 4. å†…å­˜å’Œç¼“å­˜æµ‹è¯•
    print("\n4. ğŸ’¾ ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    cache_test_results = {}
    
    # æµ‹è¯•é‡å¤æŸ¥è¯¢ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
    print("  æµ‹è¯•ç¼“å­˜å‘½ä¸­æ€§èƒ½...")
    cache_times = []
    for _ in range(20):
        response_time = client.measure_api_response_time("/api/v1/command-filter/commands?page=1&page_size=10")
        if response_time > 0:
            cache_times.append(response_time)
    
    if cache_times:
        cache_test_results["repeated_queries"] = {
            "avg_ms": statistics.mean(cache_times),
            "min_ms": min(cache_times),
            "max_ms": max(cache_times),
            "std_dev": statistics.stdev(cache_times) if len(cache_times) > 1 else 0
        }
        print(f"    ç¼“å­˜æŸ¥è¯¢å¹³å‡æ—¶é—´: {statistics.mean(cache_times):.2f}ms")
        print(f"    æ ‡å‡†å·®: {statistics.stdev(cache_times) if len(cache_times) > 1 else 0:.2f}ms")
    
    results["cache"] = cache_test_results
    
    # 5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print("\n5. ğŸ“‹ ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    print("\næ¸…ç†æµ‹è¯•æ•°æ®...")
    for policy_id in policy_ids[-5:]:  # åªæ¸…ç†æœ€å5ä¸ªï¼Œé¿å…åˆ é™¤è¿‡å¤š
        try:
            client.session.delete(f"{client.base_url}/api/v1/command-filter/policies/{policy_id}")
        except:
            pass
    
    for command_id in command_ids[-10:]:  # åªæ¸…ç†æœ€å10ä¸ª
        try:
            client.session.delete(f"{client.base_url}/api/v1/command-filter/commands/{command_id}")
        except:
            pass
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    results["test_info"] = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "created_commands": len(command_ids),
        "created_policies": len(policy_ids),
        "test_duration": "çº¦5-10åˆ†é’Ÿ"
    }
    
    with open('.specs/å‘½ä»¤ç­–ç•¥åŠŸèƒ½å¼€å‘/performance-test-6.3.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæ±‡æ€»ç»“æœ
    print("\n=== æ€§èƒ½æµ‹è¯•æ±‡æ€» ===")
    
    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ€§èƒ½è¦æ±‚ (<10ms)
    all_passed = True
    performance_issues = []
    
    if "baseline" in results:
        for test_name, metrics in results["baseline"].items():
            avg_time = metrics["avg_ms"]
            status = "âœ…" if avg_time < 10 else "âŒ"
            print(f"{status} {test_name}: {avg_time:.2f}ms")
            if avg_time >= 10:
                all_passed = False
                performance_issues.append(f"{test_name}: {avg_time:.2f}ms")
    
    if "concurrent" in results:
        for test_name, metrics in results["concurrent"].items():
            if "error" not in metrics:
                avg_time = metrics["avg_response_time_ms"]
                p95_time = metrics["p95_response_time_ms"]
                status = "âœ…" if p95_time < 10 else "âŒ"
                print(f"{status} {test_name} P95: {p95_time:.2f}ms (å¹³å‡: {avg_time:.2f}ms)")
                if p95_time >= 10:
                    all_passed = False
                    performance_issues.append(f"{test_name} P95: {p95_time:.2f}ms")
    
    print(f"\næ€»ä½“æ€§èƒ½è¯„ä¼°: {'âœ… é€šè¿‡' if all_passed else 'âŒ éœ€è¦ä¼˜åŒ–'}")
    if performance_issues:
        print("éœ€è¦å…³æ³¨çš„æ€§èƒ½é—®é¢˜:")
        for issue in performance_issues:
            print(f"  - {issue}")
    
    print(f"\nè¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: performance-test-6.3.json")
    print(f"æµ‹è¯•æ•°æ®: åˆ›å»ºäº†{len(command_ids)}ä¸ªå‘½ä»¤, {len(policy_ids)}ä¸ªç­–ç•¥")

if __name__ == "__main__":
    main()