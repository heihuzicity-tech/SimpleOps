# Bastion ä¼šè¯æ¸…ç†æœºåˆ¶ä¼˜åŒ–å®æ–½æŒ‡å—

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

### æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡æå‡
- **æ¸…ç†å»¶è¿Ÿ**: ä» 15 åˆ†é’Ÿ â†’ 10 ç§’ä»¥å†…
- **æ£€æµ‹å‡†ç¡®ç‡**: ä» 60% â†’ 95% ä»¥ä¸Š  
- **ç³»ç»Ÿååé‡**: æå‡ 300%
- **èµ„æºä½¿ç”¨**: é™ä½ 40%

### ç”¨æˆ·ä½“éªŒæ”¹è¿›
- **ä¼šè¯çŠ¶æ€å®æ—¶åŒæ­¥**: 100% å‡†ç¡®
- **å¼‚å¸¸ä¼šè¯å¿«é€Ÿæ¸…ç†**: 30 ç§’å†…
- **ç›‘æ§å“åº”é€Ÿåº¦**: æ¯«ç§’çº§

## ğŸ“‹ åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### Phase 1: ç«‹å³æ”¹è¿› (1-2å¤©) ğŸ”¥

#### 1.1 ä¼˜åŒ–å½“å‰æ¸…ç†é€»è¾‘

**æ–‡ä»¶**: `backend/services/monitor_service.go`
**ç›®æ ‡**: å‡å°‘æ•°æ®åº“æŸ¥è¯¢æ¬¡æ•°ï¼Œæé«˜æ¸…ç†æ•ˆç‡

```go
// 1. ä¼˜åŒ– updateSessionStatus æ–¹æ³•
func (m *MonitorService) updateSessionStatus() {
    now := time.Now()
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„æ¸…ç†ç­–ç•¥
    // ç«‹å³æ¸…ç†ï¼šæ˜ç¡®å·²æ–­å¼€çš„ä¼šè¯ (5ç§’)
    immediateCleanupTime := now.Add(-5 * time.Second)
    immediateResult := m.db.Model(&models.SessionRecord{}).
        Where("status = ? AND updated_at < ? AND end_time IS NULL", "active", immediateCleanupTime).
        Updates(map[string]interface{}{
            "status":     "closed",
            "end_time":   now,
            "updated_at": now,
            "close_reason": "auto_cleanup_immediate",
        })
    
    // å®šæœŸæ¸…ç†ï¼šè¶…è¿‡30ç§’æ— æ´»åŠ¨çš„ä¼šè¯
    regularCleanupTime := now.Add(-30 * time.Second)  
    regularResult := m.db.Model(&models.SessionRecord{}).
        Where("status = ? AND updated_at < ? AND end_time IS NULL", "active", regularCleanupTime).
        Updates(map[string]interface{}{
            "status":     "timeout", 
            "end_time":   now,
            "updated_at": now,
            "close_reason": "auto_cleanup_timeout",
        })
    
    totalCleaned := immediateResult.RowsAffected + regularResult.RowsAffected
    if totalCleaned > 0 {
        logrus.WithFields(logrus.Fields{
            "immediate_cleaned": immediateResult.RowsAffected,
            "timeout_cleaned":   regularResult.RowsAffected,
            "total_cleaned":     totalCleaned,
        }).Info("ä¼šè¯æ¸…ç†å®Œæˆ")
        
        // ç«‹å³åŒæ­¥åˆ°å‰ç«¯
        m.broadcastCleanupUpdate(totalCleaned)
    }
}

// æ–°å¢ï¼šæ‰¹é‡æ¸…ç†å¹¿æ’­
func (m *MonitorService) broadcastCleanupUpdate(cleanedCount int64) {
    if GlobalWebSocketService != nil {
        updateMsg := WSMessage{
            Type: "batch_cleanup_update",
            Data: map[string]interface{}{
                "cleaned_count": cleanedCount,
                "timestamp":     time.Now(),
                "trigger":       "auto_cleanup",
            },
            Timestamp: time.Now(),
        }
        
        data, _ := json.Marshal(updateMsg)
        GlobalWebSocketService.manager.broadcast <- data
    }
}
```

#### 1.2 å®ç°æ‰¹å¤„ç†æœºåˆ¶

**æ–°æ–‡ä»¶**: `backend/services/batch_cleanup_service.go`

```go
package services

import (
    "context"
    "sync"
    "time"
    "github.com/sirupsen/logrus"
)

// BatchCleanupService æ‰¹é‡æ¸…ç†æœåŠ¡
type BatchCleanupService struct {
    db                *gorm.DB
    redisClient       *redis.Client
    batchSize         int
    batchTimeout      time.Duration
    pendingCleanups   []string
    mutex             sync.Mutex
    ticker            *time.Ticker
    stopChan          chan struct{}
}

// NewBatchCleanupService åˆ›å»ºæ‰¹é‡æ¸…ç†æœåŠ¡
func NewBatchCleanupService(db *gorm.DB, redisClient *redis.Client) *BatchCleanupService {
    service := &BatchCleanupService{
        db:              db,
        redisClient:     redisClient,
        batchSize:       100,       // æ‰¹å¤„ç†å¤§å°
        batchTimeout:    5 * time.Second,  // æ‰¹å¤„ç†è¶…æ—¶
        pendingCleanups: make([]string, 0),
        stopChan:        make(chan struct{}),
    }
    
    // å¯åŠ¨æ‰¹å¤„ç†å®šæ—¶å™¨
    service.startBatchProcessor()
    return service
}

// AddCleanupTask æ·»åŠ æ¸…ç†ä»»åŠ¡
func (bcs *BatchCleanupService) AddCleanupTask(sessionID string) {
    bcs.mutex.Lock()
    defer bcs.mutex.Unlock()
    
    bcs.pendingCleanups = append(bcs.pendingCleanups, sessionID)
    
    // å¦‚æœè¾¾åˆ°æ‰¹å¤„ç†å¤§å°ï¼Œç«‹å³å¤„ç†
    if len(bcs.pendingCleanups) >= bcs.batchSize {
        go bcs.processBatch()
    }
}

// processBatch å¤„ç†æ‰¹é‡æ¸…ç†
func (bcs *BatchCleanupService) processBatch() {
    bcs.mutex.Lock()
    if len(bcs.pendingCleanups) == 0 {
        bcs.mutex.Unlock()
        return
    }
    
    // å¤åˆ¶å¾…å¤„ç†çš„ä¼šè¯ID
    sessionIDs := make([]string, len(bcs.pendingCleanups))
    copy(sessionIDs, bcs.pendingCleanups)
    bcs.pendingCleanups = bcs.pendingCleanups[:0] // æ¸…ç©º
    bcs.mutex.Unlock()
    
    start := time.Now()
    
    // å¹¶è¡Œæ‰§è¡Œæ¸…ç†æ“ä½œ
    var wg sync.WaitGroup
    errChan := make(chan error, 3)
    
    // 1. æ‰¹é‡æ›´æ–°æ•°æ®åº“
    wg.Add(1)
    go func() {
        defer wg.Done()
        errChan <- bcs.batchUpdateDatabase(sessionIDs)
    }()
    
    // 2. æ‰¹é‡æ¸…ç†Redis
    wg.Add(1)
    go func() {
        defer wg.Done()
        errChan <- bcs.batchCleanupRedis(sessionIDs)
    }()
    
    // 3. æ‰¹é‡å¹¿æ’­WebSocket
    wg.Add(1)
    go func() {
        defer wg.Done()
        errChan <- bcs.batchBroadcastCleanup(sessionIDs)
    }()
    
    wg.Wait()
    close(errChan)
    
    // ç»Ÿè®¡é”™è¯¯
    errorCount := 0
    for err := range errChan {
        if err != nil {
            errorCount++
            logrus.WithError(err).Error("æ‰¹é‡æ¸…ç†éƒ¨åˆ†æ“ä½œå¤±è´¥")
        }
    }
    
    duration := time.Since(start)
    logrus.WithFields(logrus.Fields{
        "session_count": len(sessionIDs),
        "duration_ms":   duration.Milliseconds(),
        "errors":        errorCount,
        "throughput":    float64(len(sessionIDs)) / duration.Seconds(),
    }).Info("æ‰¹é‡æ¸…ç†å®Œæˆ")
}

// batchUpdateDatabase æ‰¹é‡æ›´æ–°æ•°æ®åº“
func (bcs *BatchCleanupService) batchUpdateDatabase(sessionIDs []string) error {
    if len(sessionIDs) == 0 {
        return nil
    }
    
    now := time.Now()
    
    // ä½¿ç”¨ CASE WHEN è¿›è¡Œç²¾ç¡®æ›´æ–°
    query := `
        UPDATE session_records 
        SET 
            status = 'closed',
            end_time = ?,
            updated_at = ?,
            close_reason = 'batch_cleanup'
        WHERE session_id IN (` + strings.Repeat("?,", len(sessionIDs)-1) + `?)
            AND status = 'active'
            AND end_time IS NULL`
    
    args := []interface{}{now, now}
    for _, sessionID := range sessionIDs {
        args = append(args, sessionID)
    }
    
    result := bcs.db.Exec(query, args...)
    if result.Error != nil {
        return fmt.Errorf("æ‰¹é‡æ›´æ–°æ•°æ®åº“å¤±è´¥: %w", result.Error)
    }
    
    return nil
}

// batchCleanupRedis æ‰¹é‡æ¸…ç†Redis
func (bcs *BatchCleanupService) batchCleanupRedis(sessionIDs []string) error {
    if len(sessionIDs) == 0 {
        return nil
    }
    
    ctx := context.Background()
    pipe := bcs.redisClient.Pipeline()
    
    // æ‰¹é‡åˆ é™¤ä¼šè¯æ•°æ®å’Œæ´»è·ƒæ ‡è®°
    for _, sessionID := range sessionIDs {
        sessionKey := fmt.Sprintf("bastion:session:%s", sessionID)
        pipe.Del(ctx, sessionKey)
        pipe.SRem(ctx, "bastion:active_sessions", sessionID)
    }
    
    _, err := pipe.Exec(ctx)
    return err
}

// startBatchProcessor å¯åŠ¨æ‰¹å¤„ç†å™¨
func (bcs *BatchCleanupService) startBatchProcessor() {
    bcs.ticker = time.NewTicker(bcs.batchTimeout)
    
    go func() {
        for {
            select {
            case <-bcs.ticker.C:
                bcs.processBatch()
            case <-bcs.stopChan:
                bcs.ticker.Stop()
                return
            }
        }
    }()
}
```

#### 1.3 æ™ºèƒ½é˜ˆå€¼é…ç½®

**æ–‡ä»¶**: `backend/config/config.yaml`

```yaml
# æ–°å¢æ™ºèƒ½ä¼šè¯ç®¡ç†é…ç½®
intelligent_session:
  enable: true
  
  # æ™ºèƒ½æ£€æµ‹é…ç½®
  detection:
    heartbeat_interval: 5s      # å¿ƒè·³æ£€æµ‹é—´éš”
    health_check_interval: 10s  # å¥åº·æ£€æŸ¥é—´éš”
    idle_timeout: 30s           # ç©ºé—²è¶…æ—¶
    zombie_timeout: 60s         # åƒµå°¸ä¼šè¯è¶…æ—¶
    
  # æ‰¹å¤„ç†é…ç½®  
  batch_processing:
    enable: true
    batch_size: 100             # æ‰¹å¤„ç†å¤§å°
    batch_timeout: 5s           # æ‰¹å¤„ç†è¶…æ—¶
    max_concurrency: 5          # æœ€å¤§å¹¶å‘æ•°
    
  # è‡ªé€‚åº”é…ç½®
  adaptive:
    enable: true
    load_threshold: 0.8         # è´Ÿè½½é˜ˆå€¼
    auto_scale: true            # è‡ªåŠ¨ç¼©æ”¾
    min_interval: 5s            # æœ€å°æ¸…ç†é—´éš”
    max_interval: 60s           # æœ€å¤§æ¸…ç†é—´éš”
    
  # æ€§èƒ½ä¼˜åŒ–
  performance:
    cache_size: 10000           # ç¼“å­˜å¤§å°
    cache_ttl: 30s              # ç¼“å­˜TTL
    enable_metrics: true        # å¯ç”¨æŒ‡æ ‡æ”¶é›†
    metrics_interval: 30s       # æŒ‡æ ‡æ”¶é›†é—´éš”
```

### Phase 2: çŸ­æœŸä¼˜åŒ– (1å‘¨) ğŸš€

#### 2.1 æ™ºèƒ½çŠ¶æ€æ£€æµ‹æœåŠ¡

**æ–°æ–‡ä»¶**: `backend/services/intelligent_session_detector.go`

[æ­¤å¤„åŒ…å«å‰é¢æä¾›çš„æ™ºèƒ½æ£€æµ‹ä»£ç ]

#### 2.2 è‡ªé€‚åº”è°ƒåº¦å™¨

**æ–°æ–‡ä»¶**: `backend/services/adaptive_cleanup_scheduler.go`

```go
package services

import (
    "math"
    "time"
    "github.com/sirupsen/logrus"
)

// AdaptiveCleanupScheduler è‡ªé€‚åº”æ¸…ç†è°ƒåº¦å™¨
type AdaptiveCleanupScheduler struct {
    systemMonitor      *SystemMonitor
    performanceTracker *PerformanceTracker
    configManager      *ConfigManager
    
    currentInterval    time.Duration
    lastAdjustment     time.Time
    adjustmentHistory  []IntervalAdjustment
}

type IntervalAdjustment struct {
    Timestamp     time.Time     `json:"timestamp"`
    OldInterval   time.Duration `json:"old_interval"`
    NewInterval   time.Duration `json:"new_interval"`
    Reason        string        `json:"reason"`
    Metrics       SystemMetrics `json:"metrics"`
}

type SystemMetrics struct {
    CPUUsage          float64 `json:"cpu_usage"`
    MemoryUsage       float64 `json:"memory_usage"`
    ActiveSessions    int64   `json:"active_sessions"`
    CleanupLatency    float64 `json:"cleanup_latency_ms"`
    ErrorRate         float64 `json:"error_rate"`
    ThroughputQPS     float64 `json:"throughput_qps"`
}

// AdjustCleanupInterval è‡ªé€‚åº”è°ƒæ•´æ¸…ç†é—´éš”
func (acs *AdaptiveCleanupScheduler) AdjustCleanupInterval() time.Duration {
    metrics := acs.systemMonitor.GetCurrentMetrics()
    
    // åŸºç¡€é—´éš”ï¼ˆä»é…ç½®è·å–ï¼‰
    baseInterval := acs.configManager.GetBaseCleanupInterval()
    
    // è®¡ç®—è°ƒæ•´å› å­
    adjustmentFactor := acs.calculateAdjustmentFactor(metrics)
    
    // åº”ç”¨è°ƒæ•´
    newInterval := time.Duration(float64(baseInterval) * adjustmentFactor)
    
    // é™åˆ¶èŒƒå›´
    minInterval := acs.configManager.GetMinCleanupInterval()
    maxInterval := acs.configManager.GetMaxCleanupInterval()
    
    if newInterval < minInterval {
        newInterval = minInterval
    } else if newInterval > maxInterval {
        newInterval = maxInterval
    }
    
    // è®°å½•è°ƒæ•´å†å²
    if newInterval != acs.currentInterval {
        adjustment := IntervalAdjustment{
            Timestamp:   time.Now(),
            OldInterval: acs.currentInterval,
            NewInterval: newInterval,
            Reason:      acs.getAdjustmentReason(metrics),
            Metrics:     metrics,
        }
        
        acs.adjustmentHistory = append(acs.adjustmentHistory, adjustment)
        acs.currentInterval = newInterval
        acs.lastAdjustment = time.Now()
        
        logrus.WithFields(logrus.Fields{
            "old_interval": adjustment.OldInterval,
            "new_interval": adjustment.NewInterval,
            "reason":       adjustment.Reason,
            "cpu_usage":    metrics.CPUUsage,
            "memory_usage": metrics.MemoryUsage,
            "sessions":     metrics.ActiveSessions,
        }).Info("æ¸…ç†é—´éš”å·²è‡ªé€‚åº”è°ƒæ•´")
    }
    
    return newInterval
}

// calculateAdjustmentFactor è®¡ç®—è°ƒæ•´å› å­
func (acs *AdaptiveCleanupScheduler) calculateAdjustmentFactor(metrics SystemMetrics) float64 {
    factor := 1.0
    
    // CPU ä½¿ç”¨ç‡å› å­
    cpuFactor := 1.0
    if metrics.CPUUsage > 0.8 {
        cpuFactor = 1.5  // CPUé«˜æ—¶å‡å°‘æ¸…ç†é¢‘ç‡
    } else if metrics.CPUUsage < 0.3 {
        cpuFactor = 0.7  // CPUä½æ—¶å¢åŠ æ¸…ç†é¢‘ç‡
    }
    
    // å†…å­˜ä½¿ç”¨ç‡å› å­
    memoryFactor := 1.0
    if metrics.MemoryUsage > 0.8 {
        memoryFactor = 1.3
    } else if metrics.MemoryUsage < 0.4 {
        memoryFactor = 0.8
    }
    
    // ä¼šè¯æ•°é‡å› å­
    sessionFactor := 1.0
    if metrics.ActiveSessions > 1000 {
        sessionFactor = 0.6  // ä¼šè¯å¤šæ—¶å¢åŠ æ¸…ç†é¢‘ç‡
    } else if metrics.ActiveSessions < 100 {
        sessionFactor = 1.2  // ä¼šè¯å°‘æ—¶å‡å°‘æ¸…ç†é¢‘ç‡
    }
    
    // é”™è¯¯ç‡å› å­
    errorFactor := 1.0
    if metrics.ErrorRate > 0.05 {
        errorFactor = 1.4  // é”™è¯¯ç‡é«˜æ—¶å‡å°‘æ¸…ç†é¢‘ç‡
    }
    
    // å»¶è¿Ÿå› å­
    latencyFactor := 1.0
    if metrics.CleanupLatency > 5000 { // 5ç§’
        latencyFactor = 1.3
    } else if metrics.CleanupLatency < 1000 { // 1ç§’
        latencyFactor = 0.8
    }
    
    // ç»¼åˆè®¡ç®— - ä½¿ç”¨å‡ ä½•å¹³å‡é¿å…æç«¯å€¼
    factors := []float64{cpuFactor, memoryFactor, sessionFactor, errorFactor, latencyFactor}
    geometricMean := 1.0
    for _, f := range factors {
        geometricMean *= f
    }
    factor = math.Pow(geometricMean, 1.0/float64(len(factors)))
    
    // é™åˆ¶è°ƒæ•´å¹…åº¦ï¼Œé¿å…éœ‡è¡
    if factor > 2.0 {
        factor = 2.0
    } else if factor < 0.5 {
        factor = 0.5
    }
    
    return factor
}

// getAdjustmentReason è·å–è°ƒæ•´åŸå› 
func (acs *AdaptiveCleanupScheduler) getAdjustmentReason(metrics SystemMetrics) string {
    reasons := []string{}
    
    if metrics.CPUUsage > 0.8 {
        reasons = append(reasons, "high_cpu")
    }
    if metrics.MemoryUsage > 0.8 {
        reasons = append(reasons, "high_memory") 
    }
    if metrics.ActiveSessions > 1000 {
        reasons = append(reasons, "high_sessions")
    }
    if metrics.ErrorRate > 0.05 {
        reasons = append(reasons, "high_error_rate")
    }
    if metrics.CleanupLatency > 5000 {
        reasons = append(reasons, "high_latency")
    }
    
    if len(reasons) == 0 {
        return "optimization"
    }
    
    return strings.Join(reasons, ",")
}
```

### Phase 3: ä¸­æœŸé‡æ„ (2-3å‘¨) ğŸ—ï¸

#### 3.1 å®Œæ•´çš„ä¼šè¯ç®¡ç†å™¨é‡æ„

**æ–°æ–‡ä»¶**: `backend/services/enhanced_session_manager.go`

[æ­¤å¤„åŒ…å«å‰é¢æä¾›çš„å¢å¼ºä¼šè¯ç®¡ç†å™¨ä»£ç ]

#### 3.2 æœºå™¨å­¦ä¹ ç®—æ³•é›†æˆ

**æ–°æ–‡ä»¶**: `backend/ml/session_prediction_model.go`

```go
package ml

import (
    "math"
    "time"
    "gonum.org/v1/gonum/mat"
)

// SessionPredictionModel ä¼šè¯é¢„æµ‹æ¨¡å‹
type SessionPredictionModel struct {
    weights          *mat.Dense
    featureNormalizer *FeatureNormalizer
    trained          bool
    trainingData     []TrainingExample
    accuracy         float64
}

type TrainingExample struct {
    Features []float64 `json:"features"`
    Label    bool       `json:"label"` // true = should cleanup, false = keep active
}

type SessionFeatures struct {
    TimeSinceLastActivity   float64 `json:"time_since_last_activity"`   // ç§’
    TimeSinceLastHeartbeat  float64 `json:"time_since_last_heartbeat"`  // ç§’
    CommandsInLastMinute    float64 `json:"commands_in_last_minute"`
    DataTransferredRecently float64 `json:"data_transferred_recently"`  // å­—èŠ‚
    SessionDuration         float64 `json:"session_duration"`           // ç§’
    TimeOfDay               float64 `json:"time_of_day"`                // 0-24å°æ—¶
    DayOfWeek               float64 `json:"day_of_week"`                // 0-6
    UserHistoricalBehavior  float64 `json:"user_historical_behavior"`   // ç”¨æˆ·è¡Œä¸ºæ¨¡å¼è¯„åˆ†
    AssetType               float64 `json:"asset_type"`                 // èµ„äº§ç±»å‹ç¼–ç 
    NetworkLatency          float64 `json:"network_latency"`            // æ¯«ç§’
}

// ExtractFeatures æå–ä¼šè¯ç‰¹å¾
func (spm *SessionPredictionModel) ExtractFeatures(sessionID string, healthInfo *SessionHealthInfo) *SessionFeatures {
    now := time.Now()
    
    features := &SessionFeatures{
        TimeSinceLastActivity:   now.Sub(healthInfo.LastActivity).Seconds(),
        TimeSinceLastHeartbeat:  now.Sub(healthInfo.LastHeartbeat).Seconds(),
        CommandsInLastMinute:    float64(healthInfo.Metrics.CommandsExecuted),
        DataTransferredRecently: float64(healthInfo.Metrics.DataTransferred),
        SessionDuration:         now.Sub(healthInfo.Metrics.SessionStartTime).Seconds(),
        TimeOfDay:               float64(now.Hour()) + float64(now.Minute())/60.0,
        DayOfWeek:               float64(now.Weekday()),
        UserHistoricalBehavior:  spm.calculateUserBehaviorScore(healthInfo.UserID),
        AssetType:               spm.encodeAssetType(healthInfo.AssetID),
        NetworkLatency:          float64(healthInfo.Metrics.NetworkLatency.Milliseconds()),
    }
    
    return features
}

// Predict é¢„æµ‹æ˜¯å¦åº”è¯¥æ¸…ç†ä¼šè¯
func (spm *SessionPredictionModel) Predict(features *SessionFeatures) (shouldCleanup bool, confidence float64) {
    if !spm.trained {
        // å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨åŸºç¡€è§„åˆ™
        return spm.basicRulePrediction(features)
    }
    
    // å°†ç‰¹å¾è½¬æ¢ä¸ºå‘é‡
    featureVector := spm.featuresToVector(features)
    
    // æ ‡å‡†åŒ–ç‰¹å¾
    normalizedFeatures := spm.featureNormalizer.Normalize(featureVector)
    
    // è®¡ç®—é¢„æµ‹å€¼ (sigmoidæ¿€æ´»)
    var prediction float64
    for i := 0; i < normalizedFeatures.Len(); i++ {
        prediction += normalizedFeatures.AtVec(i) * spm.weights.At(i, 0)
    }
    
    // Sigmoidå‡½æ•°
    probability := 1.0 / (1.0 + math.Exp(-prediction))
    
    // å†³ç­–é˜ˆå€¼ 0.5
    shouldCleanup = probability > 0.5
    confidence = math.Abs(probability - 0.5) * 2 // è½¬æ¢ä¸º0-1çš„ç½®ä¿¡åº¦
    
    return shouldCleanup, confidence
}

// basicRulePrediction åŸºç¡€è§„åˆ™é¢„æµ‹ï¼ˆæœªè®­ç»ƒæ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰
func (spm *SessionPredictionModel) basicRulePrediction(features *SessionFeatures) (bool, float64) {
    score := 0.0
    
    // åŸºäºæ—¶é—´çš„è§„åˆ™
    if features.TimeSinceLastActivity > 300 { // 5åˆ†é’Ÿæ— æ´»åŠ¨
        score += 0.3
    }
    
    if features.TimeSinceLastHeartbeat > 60 { // 1åˆ†é’Ÿæ— å¿ƒè·³
        score += 0.4
    }
    
    // åŸºäºæ´»åŠ¨çš„è§„åˆ™
    if features.CommandsInLastMinute == 0 && features.DataTransferredRecently == 0 {
        score += 0.2
    }
    
    // åŸºäºæ—¶é•¿çš„è§„åˆ™
    if features.SessionDuration > 7200 { // è¶…è¿‡2å°æ—¶
        score += 0.1
    }
    
    shouldCleanup := score > 0.5
    confidence := math.Min(score, 1.0)
    
    return shouldCleanup, confidence
}

// TrainModel è®­ç»ƒæ¨¡å‹
func (spm *SessionPredictionModel) TrainModel() error {
    if len(spm.trainingData) < 100 {
        return fmt.Errorf("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘100ä¸ªæ ·æœ¬ï¼Œå½“å‰: %d", len(spm.trainingData))
    }
    
    // å‡†å¤‡è®­ç»ƒæ•°æ®
    featureMatrix, labelVector := spm.prepareTrainingData()
    
    // ä½¿ç”¨é€»è¾‘å›å½’è®­ç»ƒ
    spm.weights = spm.logisticRegression(featureMatrix, labelVector)
    
    // è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
    spm.accuracy = spm.calculateAccuracy(featureMatrix, labelVector)
    spm.trained = true
    
    logrus.WithFields(logrus.Fields{
        "training_samples": len(spm.trainingData),
        "accuracy":        spm.accuracy,
        "features":        featureMatrix.RawMatrix().Cols,
    }).Info("ä¼šè¯é¢„æµ‹æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    return nil
}

// logisticRegression é€»è¾‘å›å½’å®ç°
func (spm *SessionPredictionModel) logisticRegression(X, y *mat.Dense) *mat.Dense {
    learningRate := 0.01
    iterations := 1000
    
    m, n := X.Dims()
    weights := mat.NewDense(n, 1, nil)
    
    for i := 0; i < iterations; i++ {
        // å‰å‘ä¼ æ’­
        predictions := mat.NewDense(m, 1, nil)
        predictions.Mul(X, weights)
        
        // Sigmoidæ¿€æ´»
        for j := 0; j < m; j++ {
            val := predictions.At(j, 0)
            sigmoid := 1.0 / (1.0 + math.Exp(-val))
            predictions.Set(j, 0, sigmoid)
        }
        
        // è®¡ç®—è¯¯å·®
        errors := mat.NewDense(m, 1, nil)
        errors.Sub(predictions, y)
        
        // è®¡ç®—æ¢¯åº¦
        gradient := mat.NewDense(n, 1, nil)
        gradient.Mul(X.T(), errors)
        gradient.Scale(1.0/float64(m), gradient)
        
        // æ›´æ–°æƒé‡
        update := mat.NewDense(n, 1, nil)
        update.Scale(learningRate, gradient)
        weights.Sub(weights, update)
    }
    
    return weights
}
```

## ğŸš€ å¿«é€Ÿå¯åŠ¨å®æ–½

### ç«‹å³å¯æ‰§è¡Œçš„ä»£ç ä¿®æ”¹

1. **ç«‹å³ä¿®æ”¹ monitor_service.go**ï¼š

```bash
# å¤‡ä»½åŸæ–‡ä»¶
cp backend/services/monitor_service.go backend/services/monitor_service.go.bak

# åº”ç”¨ä¼˜åŒ–è¡¥ä¸ï¼ˆéœ€è¦æ‰‹åŠ¨ä¿®æ”¹updateSessionStatusæ–¹æ³•ï¼‰
```

2. **æ·»åŠ æ‰¹å¤„ç†æœåŠ¡**ï¼š

```bash
# åˆ›å»ºæ–°çš„æ‰¹å¤„ç†æœåŠ¡æ–‡ä»¶
touch backend/services/batch_cleanup_service.go
# å¤åˆ¶ä¸Šé¢æä¾›çš„ä»£ç 
```

3. **æ›´æ–°é…ç½®æ–‡ä»¶**ï¼š

```bash
# åœ¨config.yamlä¸­æ·»åŠ æ™ºèƒ½ä¼šè¯ç®¡ç†é…ç½®
# å¤åˆ¶ä¸Šé¢æä¾›çš„é…ç½®
```

### ç›‘æ§å’ŒéªŒè¯

1. **æ€§èƒ½ç›‘æ§**ï¼š

```bash
# ç›‘æ§æ¸…ç†æ•ˆæœ
tail -f backend/bastion-backend.log | grep -E "ä¼šè¯æ¸…ç†|batch.*cleanup|æ™ºèƒ½æ£€æµ‹"

# æ•°æ®åº“æ€§èƒ½ç›‘æ§
mysql -e "SHOW PROCESSLIST;" | grep session_records

# Redisæ€§èƒ½ç›‘æ§  
redis-cli --latency-history
```

2. **éªŒè¯æ¸…ç†æ•ˆæœ**ï¼š

```sql
-- æ£€æŸ¥æ´»è·ƒä¼šè¯æ•°é‡
SELECT COUNT(*) as active_sessions FROM session_records WHERE status = 'active';

-- æ£€æŸ¥æ¸…ç†åŸå› åˆ†å¸ƒ
SELECT close_reason, COUNT(*) as count FROM session_records 
WHERE close_reason IS NOT NULL 
GROUP BY close_reason 
ORDER BY count DESC;

-- æ£€æŸ¥å¹³å‡ä¼šè¯æŒç»­æ—¶é—´
SELECT AVG(TIMESTAMPDIFF(SECOND, start_time, end_time)) as avg_duration_seconds
FROM session_records 
WHERE end_time IS NOT NULL;
```

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æ”¹è¿›å¹…åº¦ |
|------|------|------|----------|
| æ¸…ç†å»¶è¿Ÿ | 15åˆ†é’Ÿ | 10ç§’ | 99.9% âš¡ |
| æ£€æµ‹å‡†ç¡®ç‡ | 60% | 95% | 58% ğŸ“ˆ |
| ç³»ç»Ÿååé‡ | 100 QPS | 400 QPS | 300% ğŸš€ |
| å†…å­˜ä½¿ç”¨ | é«˜ | é™ä½40% | 40% ğŸ“‰ |
| CPUä½¿ç”¨ | é«˜ | é™ä½30% | 30% ğŸ“‰ |

### ç”¨æˆ·ä½“éªŒæ”¹è¿›

- âœ… **ä¼šè¯çŠ¶æ€å®æ—¶åŒæ­¥**: 100%å‡†ç¡®ï¼Œæ— å»¶è¿Ÿ
- âœ… **å¼‚å¸¸æ£€æµ‹**: 30ç§’å†…è¯†åˆ«å¹¶æ¸…ç†åƒµå°¸ä¼šè¯  
- âœ… **æ™ºèƒ½é¢„æµ‹**: 95%å‡†ç¡®ç‡é¢„æµ‹ä¼šè¯ç”Ÿå‘½å‘¨æœŸ
- âœ… **æ‰¹é‡å¤„ç†**: å‡å°‘æ•°æ®åº“æŸ¥è¯¢90%
- âœ… **è‡ªé€‚åº”è°ƒæ•´**: æ ¹æ®ç³»ç»Ÿè´Ÿè½½è‡ªåŠ¨ä¼˜åŒ–

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

1. **æ‰¹å¤„ç†æœåŠ¡å¯åŠ¨å¤±è´¥**ï¼š
   - æ£€æŸ¥Redisè¿æ¥é…ç½®
   - ç¡®è®¤æ•°æ®åº“æƒé™
   - æŸ¥çœ‹æ—¥å¿—é”™è¯¯ä¿¡æ¯

2. **æ¸…ç†æ•ˆæœä¸æ˜æ˜¾**ï¼š
   - éªŒè¯é…ç½®æ–‡ä»¶æ›´æ–°
   - æ£€æŸ¥WebSocketæœåŠ¡çŠ¶æ€
   - ç¡®è®¤å‰ç«¯WebSocketå¤„ç†é€»è¾‘

3. **æ€§èƒ½åè€Œä¸‹é™**ï¼š
   - è°ƒæ•´æ‰¹å¤„ç†å¤§å°
   - æ£€æŸ¥å¹¶å‘é…ç½®
   - ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨

### ç´§æ€¥å›æ»šæ–¹æ¡ˆ

```bash
# å¿«é€Ÿå›æ»šåˆ°åŸå§‹çŠ¶æ€
cp backend/services/monitor_service.go.bak backend/services/monitor_service.go
rm backend/services/batch_cleanup_service.go
git checkout -- backend/config/config.yaml

# é‡å¯æœåŠ¡
./manage.sh restart backend
```

---

**å®æ–½å»ºè®®**: å»ºè®®å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯Phase 1çš„æ”¹è¿›æ•ˆæœï¼Œç¡®è®¤æ— é—®é¢˜åå†é€æ­¥æ¨è¿›åˆ°ç”Ÿäº§ç¯å¢ƒã€‚